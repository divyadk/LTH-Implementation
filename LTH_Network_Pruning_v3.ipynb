{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-story",
   "metadata": {},
   "source": [
    "### Pruning after every 50 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "athletic-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "weekly-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_epochs=2\n",
    "batch_size_train=64\n",
    "batch_size_test=1000\n",
    "learning_rate=0.01\n",
    "momentum=0.5 #momentum will use the gradient of past steps also instead of just the current gradient\n",
    "log_interval=1000\n",
    "\n",
    "random_seed=1 #seed means that the random number generation will start from this point only and hence same numbers will be generated\n",
    "torch.backends.cudnn.enabled=False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "#loading the dataset\n",
    "train_loader=torch.utils.data.DataLoader(torchvision.datasets.MNIST(root='./data/files_MNIST/',train=True,download=False,transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size=batch_size_train,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torchvision.datasets.MNIST(root='./data/files_MNIST/', train=False, download=False,transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.1307,),(0.3081,))])),batch_size=batch_size_test,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "comfortable-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a class for the Pruning model from concentration\n",
    "class ConcPrune(nn.Module):\n",
    "    #these are the idiotypic data\n",
    "    def __init__(self):\n",
    "        super(ConcPrune,self).__init__()\n",
    "        self.channels_conv1=64\n",
    "        self.channels_conv2=64\n",
    "        self.ksize_conv1=3\n",
    "        self.ksize_conv2=3\n",
    "        self.fc_postpool=14*14*64\n",
    "        self.h3ln=256\n",
    "        self.h4ln=256\n",
    "        self.out_classes=10\n",
    "        self.init_conc=100.0\n",
    "        \n",
    "        \n",
    "    def prune_by_conc(self):\n",
    "        print('pruning by conc')\n",
    "\n",
    "\n",
    "        #identifying the non hot least concentrated neurons here - only one population - not multiple\n",
    "        #h1 and h2 are convolutional layers and h3 is a linear layer\n",
    "\n",
    "        #add other functionalities here - send the conc from the other func\n",
    "        #calculate the min and max conc weights - that is - determine hot and non hot neurons\n",
    "    def prune_by_lth(self,reinit=False):\n",
    "        print('Pruning by LTH')\n",
    "        s=0.25\n",
    "        self.conv1.lth_prune(reinit)\n",
    "        self.conv2.lth_prune(reinit)\n",
    "        self.fc1.lth_prune(reinit)\n",
    "        self.fc2.lth_prune(reinit)\n",
    "        self.fc3.lth_prune(reinit)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "resident-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a class for the Masked Layers - conv\n",
    "class MaskConv(nn.Module):\n",
    "    def __init__(self, inc, outc, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False):\n",
    "        super(MaskConv,self).__init__()\n",
    "        self.inc=inc\n",
    "        self.outc=outc\n",
    "        self.kernel_size= kernel_size\n",
    "        self.stride=stride\n",
    "        self.padding=padding\n",
    "        self.dilation=dilation\n",
    "        self.groups=groups\n",
    "        self.pruning_fraction=0.2\n",
    "        \n",
    "        self.weights_cust=Parameter(torch.Tensor(self.outc,self.inc,*self.kernel_size))\n",
    "        self.mask=Parameter(torch.ones([self.outc,self.inc,*self.kernel_size]),requires_grad=False)\n",
    "        if bias:\n",
    "            self.bias=Parameter(torch.Tensor(outc))\n",
    "        else:\n",
    "            self.register_parameter('bias',None)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    #this is to reset the parameters - initialise the weights\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weights_cust,a=math.sqrt(5))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return F.conv2d(input, self.weights_cust*self.mask, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "            \n",
    "    \n",
    "    #here pruning by the lottery ticket hypothesis\n",
    "    def lth_prune(self,reinit=False):\n",
    "        prune_val=0.1 #this is equivalent to 10%\n",
    "        weights = self.weights_cust.data.clone().cpu().detach().numpy()\n",
    "        tensor_mask=self.mask.data.clone().cpu().detach().numpy()\n",
    "        \n",
    "        number_of_remaining_weights = np.sum(tensor_mask)\n",
    "        number_of_weights_to_prune = np.ceil(prune_val * number_of_remaining_weights).astype(int)\n",
    "        weight_vector=np.concatenate([weights[tensor_mask == 1]])\n",
    "        threshold = np.sort(np.abs(weight_vector))[number_of_weights_to_prune]\n",
    "        new_mask =np.where(np.abs(weights) > threshold, tensor_mask, np.zeros_like(weights))\n",
    "\n",
    "        self.mask=torch.nn.Parameter(torch.from_numpy(new_mask),requires_grad=False)\n",
    "        print(self.mask)\n",
    "        \n",
    "        #prune_val=10\n",
    "        #prune_percentile=np.percentile(abs(alive), prune_val)\n",
    "        #threshold=prune_percentile\n",
    "        # Convert Tensors to numpy and calculate\n",
    "        \n",
    "        \n",
    "        if reinit:\n",
    "            nn.init.xavier_uniform_(self.weights_cust)\n",
    "            self.weights_cust.data = self.weights_cust.data * self.mask.data\n",
    "        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "minor-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a class for the masked layers - linear\n",
    "class MaskLin(Module):\n",
    "    def __init__(self, inf, outf, bias=True):\n",
    "        super(MaskLin,self).__init__()\n",
    "        self.inf=inf\n",
    "        self.outf=outf\n",
    "        self.weight=Parameter(torch.Tensor(outf, inf))\n",
    "        self.mask = Parameter(torch.ones([outf, inf]), requires_grad=False)\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(outf))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input,self.weight*self.mask, self.bias)\n",
    "    \n",
    "    \n",
    "    def lth_prune(self,reinit=False):\n",
    "        prune_val=0.12 #this is equivalent to 20% for the fc layers\n",
    "        weights = self.weight.data.clone().cpu().detach().numpy()\n",
    "        tensor_mask=self.mask.data.clone().cpu().detach().numpy()\n",
    "        \n",
    "        number_of_remaining_weights = np.sum(tensor_mask)\n",
    "        number_of_weights_to_prune = np.ceil(prune_val * number_of_remaining_weights).astype(int)\n",
    "        weight_vector=np.concatenate([weights[tensor_mask == 1]])\n",
    "        threshold = np.sort(np.abs(weight_vector))[number_of_weights_to_prune]\n",
    "        new_mask =np.where(np.abs(weights) > threshold, tensor_mask, np.zeros_like(weights))\n",
    "\n",
    "        self.mask=torch.nn.Parameter(torch.from_numpy(new_mask),requires_grad=False)\n",
    "        \n",
    "        if reinit:\n",
    "            nn.init.xavier_uniform_(self.weight)\n",
    "            self.weight.data = self.weight.data * self.mask.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "civilian-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a class for the conv network\n",
    "class Net(ConcPrune):\n",
    "    def __init__(self,mask=True):\n",
    "        super(Net,self).__init__()\n",
    "        conv=MaskConv if mask else nn.Conv2d\n",
    "        lin=MaskLin if mask else nn.Linear\n",
    "        self.conv1 = conv(1, 64, kernel_size=(3, 3))\n",
    "        self.conv2 = conv(64, 64, kernel_size=(3, 3))\n",
    "        #self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = lin(12*12*64,256)\n",
    "        self.fc2 = lin(256, 256)\n",
    "        self.fc3 = lin(256, 10)\n",
    "\n",
    "        #all others are left out as of now        \n",
    "\n",
    "        ##check whether you have to manually initialise the weights or not\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x=F.max_pool2d(x,kernel_size=2)\n",
    "        #x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1,12*12*64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x= self.fc3(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alternative-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising the network and the optimizer\n",
    "nnetwork = Net()\n",
    "#network.to(device)\n",
    "noptimizer = optim.SGD(nnetwork.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nnetwork.fc1.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "masterfile_init='../Logs/initial_T1/common/'\n",
    "torch.save(nnetwork.state_dict(),masterfile_init+'init_weights_first.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "flexible-cookie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masterfile_init='../Logs/initial_T1/common/'\n",
    "nnetwork.load_state_dict(torch.load(masterfile_init+'init_weights_first.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "closing-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(network):\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      #data=data.to(device)\n",
    "      #target=target.to(device)\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      #print(pred)\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.6f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "  test_log_list.append([test_loss,correct,100. * correct / len(test_loader.dataset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hearing-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch,network,optimizer):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #print(batch_idx)\n",
    "        #data=data.to(device)\n",
    "        #target=target.to(device)\n",
    "        #if batch_idx==100:\n",
    "        #    break\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        #this is where the backpropagation carries out\n",
    "        loss.backward()\n",
    "        #l1grad=network.conv1.weight.grad.clone()\n",
    "        #print(l1grad)\n",
    "        #pickle.dump(l1grad,testout)\n",
    "        network.conv1.weights_cust.grad=network.conv1.weights_cust.grad*network.conv1.mask\n",
    "        network.conv2.weights_cust.grad=network.conv2.weights_cust.grad*network.conv2.mask\n",
    "        network.fc1.weight.grad=network.fc1.weight.grad*network.fc1.mask\n",
    "        network.fc2.weight.grad=network.fc2.weight.grad*network.fc2.mask\n",
    "\n",
    "        '''l1grad=network.conv1.weights_cust.grad.clone()\n",
    "        l2grad=network.conv2.weights_cust.grad.clone()\n",
    "        #print('l2grad is ',l2grad.shape)\n",
    "        l3grad=network.fc1.weight.grad.clone()\n",
    "        #print('l3grad is ',l3grad.shape)\n",
    "        l4grad=network.fc2.weight.grad.clone()\n",
    "\n",
    "        #print(grad_dic)\n",
    "        #torch.save(grad_dic,'grad2.pt')\n",
    "        gradconv1_list.append(l1grad)\n",
    "        gradconv2_list.append(l2grad)\n",
    "        gradfc1_list.append(l3grad)\n",
    "        gradfc2_list.append(l4grad)'''\n",
    "        train_losses.append(loss.item())\n",
    "        #buffer = io.BytesIO()\n",
    "        #torch.save(l1grad, buffer)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx==35 and prune:\n",
    "            network=network.prune_by_lth(reinit=False)\n",
    "            network.load_state_dict(torch.load(masterfile_init+'init_weights_first.pth'))\n",
    "            \n",
    "        if batch_idx==36 and prune:\n",
    "            print(network.conv1.mask)\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "          train_counter.append(\n",
    "            (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "    return network,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "compressed-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "test_log_list=[]\n",
    "\n",
    "gradconv1_list=[]\n",
    "gradconv2_list=[]\n",
    "gradfc1_list=[]\n",
    "gradfc2_list=[]\n",
    "prune=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "important-extension",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Research\\IN_Network_Pruning\\pbenv\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "D:\\Research\\IN_Network_Pruning\\pbenv\\lib\\site-packages\\torch\\nn\\_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3041, Accuracy: 767/10000 (7.670000%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(nnetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "foreign-disability",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Research\\IN_Network_Pruning\\pbenv\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305583\n",
      "\n",
      "Test set: Avg. loss: 0.1480, Accuracy: 9522/10000 (95.220001%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1+1):\n",
    "  nnetwork,noptimizer=train(epoch,nnetwork,noptimizer)\n",
    "  test(nnetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31e8c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "masterfile_init='../Logs/initial_T1/common/'\n",
    "torch.save(train_losses,masterfile_init+'train_losses_lth.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "twelve-scheme",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(nnetwork.conv1.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "masterfile_init='../Logs/initial_T1/common/'\n",
    "torch.save(nnetwork.state_dict(),masterfile_init+'trained_weights_nostrategy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the old initialiased weights\n",
    "masterfile_init='../Logs/initial_T1/common/'\n",
    "nnetwork.load_state_dict(torch.load(masterfile_init+'init_weights_first.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "elementary-bunny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning by LTH\n",
      "Parameter containing:\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n",
      "Parameter containing:\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 0., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[0., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[0., 1., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         [[0., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [0., 1., 0.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[0., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 0., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 0., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 0.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 0.],\n",
      "          [1., 0., 0.]],\n",
      "\n",
      "         [[1., 0., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): MaskConv()\n",
       "  (conv2): MaskConv()\n",
       "  (fc1): MaskLin()\n",
       "  (fc2): MaskLin()\n",
       "  (fc3): MaskLin()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnetwork.prune_by_lth(reinit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS WORKED FOR CONV - THIS IS AS PER ORIGINAL LTH FROM FRANKLE\n",
    "\n",
    "weights = nnetwork.conv1.weights_cust.data.clone().cpu().detach().numpy()\n",
    "tensor_mask=nnetwork.conv1.mask.data.clone().cpu().detach().numpy()\n",
    "number_of_remaining_weights = np.sum(tensor_mask)\n",
    "print(number_of_remaining_weights)\n",
    "number_of_weights_to_prune = np.ceil(0.2 * number_of_remaining_weights).astype(int)\n",
    "print(number_of_weights_to_prune)\n",
    "weight_vector=np.concatenate([weights[tensor_mask == 1]])\n",
    "print(weight_vector.shape)\n",
    "threshold = np.sort(np.abs(weight_vector))[number_of_weights_to_prune]\n",
    "print(threshold)\n",
    "new_mask =np.where(np.abs(weights) > threshold, tensor_mask, np.zeros_like(weights))\n",
    "\n",
    "number_of_remaining_weights_after = np.sum(new_mask)\n",
    "print(number_of_remaining_weights_after)\n",
    "\n",
    "nnetwork.conv1.mask=torch.nn.Parameter(torch.from_numpy(new_mask),requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS WORKED FOR FC - THIS IS AS PER ORIGINAL LTH FROM FRANKLE\n",
    "\n",
    "weights = nnetwork.fc1.weight.data.clone().cpu().detach().numpy()\n",
    "tensor_mask=nnetwork.fc1.mask.data.clone().cpu().detach().numpy()\n",
    "number_of_remaining_weights = np.sum(tensor_mask)\n",
    "print(number_of_remaining_weights)\n",
    "number_of_weights_to_prune = np.ceil(0.2 * number_of_remaining_weights).astype(int)\n",
    "print(number_of_weights_to_prune)\n",
    "weight_vector=np.concatenate([weights[tensor_mask == 1]])\n",
    "print(weight_vector.shape)\n",
    "threshold = np.sort(np.abs(weight_vector))[number_of_weights_to_prune]\n",
    "print(threshold)\n",
    "new_mask =np.where(np.abs(weights) > threshold, tensor_mask, np.zeros_like(weights))\n",
    "\n",
    "number_of_remaining_weights_after = np.sum(new_mask)\n",
    "print(number_of_remaining_weights_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(nnetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 1+1):\n",
    "  nnetwork,noptimizer=train(epoch,nnetwork,noptimizer)\n",
    "  test(nnetwork)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
